{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dependencuas: nltk, numpy, networkx\n",
    "\"\"\"\n",
    "#!python2\n",
    "#coding: utf-8-sig\n",
    "\n",
    "import io\n",
    "import nltk\n",
    "#nltk.download()<----Es necesario hacerlo la primera vez\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "import networkx as nx\n",
    "import os\n",
    "import codecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#apply syntactic filters based on POS tags\n",
    "def filter_for_tags(tagged, tags=['NN', 'JJ', 'NNP']):\n",
    "    return [item for item in tagged if item[1] in tags]\n",
    "\n",
    "def normalize(tagged):\n",
    "    return [(item[0].replace('.', ''), item[1]) for item in tagged]\n",
    "\n",
    "def unique_everseen(iterable, key=None):\n",
    "    \"List unique elements, preserving order. Remember all elements ever seen.\"\n",
    "    # unique_everseen('AAAABBBCCDAABBB') --> A B C D\n",
    "    # unique_everseen('ABBCcAD', str.lower) --> A B C D\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    if key is None:\n",
    "        for element in itertools.ifilterfalse(seen.__contains__, iterable):\n",
    "            seen_add(element)\n",
    "            yield element\n",
    "    else:\n",
    "        for element in iterable:\n",
    "            k = key(element)\n",
    "            if k not in seen:\n",
    "                seen_add(k)\n",
    "                yield element\n",
    "\n",
    "def lDistance(firstString, secondString):\n",
    "    \"Function to find the Levenshtein distance between two words/sentences - gotten from http://rosettacode.org/wiki/Levenshtein_distance#Python\"\n",
    "    if len(firstString) > len(secondString):\n",
    "        firstString, secondString = secondString, firstString\n",
    "    distances = range(len(firstString) + 1)\n",
    "    for index2, char2 in enumerate(secondString):\n",
    "        newDistances = [index2 + 1]\n",
    "        for index1, char1 in enumerate(firstString):\n",
    "            if char1 == char2:\n",
    "                newDistances.append(distances[index1])\n",
    "            else:\n",
    "                newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1])))\n",
    "        distances = newDistances\n",
    "    return distances[-1]\n",
    "\n",
    "def buildGraph(nodes):\n",
    "    \"nodes - list of hashables that represents the nodes of the graph\"\n",
    "    gr = nx.Graph() #initialize an undirected graph\n",
    "    gr.add_nodes_from(nodes)\n",
    "    nodePairs = list(itertools.combinations(nodes, 2))\n",
    "\n",
    "    #add edges to the graph (weighted by Levenshtein distance)\n",
    "    for pair in nodePairs:\n",
    "        firstString = pair[0]\n",
    "        secondString = pair[1]\n",
    "        levDistance = lDistance(firstString, secondString)\n",
    "        gr.add_edge(firstString, secondString, weight=levDistance)\n",
    "\n",
    "    return gr\n",
    "\n",
    "def extractKeyphrases(text):\n",
    "    #tokenize the text using nltk\n",
    "    wordTokens = nltk.word_tokenize(text)\n",
    "\n",
    "    #assign POS tags to the words in the text\n",
    "    tagged = nltk.pos_tag(wordTokens)\n",
    "    textlist = [x[0] for x in tagged]\n",
    "    \n",
    "    tagged = filter_for_tags(tagged)\n",
    "    tagged = normalize(tagged)\n",
    "\n",
    "    unique_word_set = unique_everseen([x[0] for x in tagged])\n",
    "    word_set_list = list(unique_word_set)\n",
    "\n",
    "   #this will be used to determine adjacent words in order to construct keyphrases with two words\n",
    "\n",
    "    graph = buildGraph(word_set_list)\n",
    "\n",
    "    #pageRank - initial value of 1.0, error tolerance of 0,0001, \n",
    "    calculated_page_rank = nx.pagerank(graph, weight='weight')\n",
    "\n",
    "    #most important words in ascending order of importance\n",
    "    keyphrases = sorted(calculated_page_rank, key=calculated_page_rank.get, reverse=True)\n",
    "\n",
    "    #the number of keyphrases returned will be relative to the size of the text (a third of the number of vertices)\n",
    "    aThird = len(word_set_list) / 3\n",
    "    keyphrases = keyphrases[0:aThird+1]\n",
    "\n",
    "    #take keyphrases with multiple words into consideration as done in the paper - if two words are adjacent in the text and are selected as keywords, join them\n",
    "    #together\n",
    "    modifiedKeyphrases = set([])\n",
    "    dealtWith = set([]) #keeps track of individual keywords that have been joined to form a keyphrase\n",
    "    i = 0\n",
    "    j = 1\n",
    "    while j < len(textlist):\n",
    "        firstWord = textlist[i]\n",
    "        secondWord = textlist[j]\n",
    "        if firstWord in keyphrases and secondWord in keyphrases:\n",
    "            keyphrase = firstWord + ' ' + secondWord\n",
    "            modifiedKeyphrases.add(keyphrase)\n",
    "            dealtWith.add(firstWord)\n",
    "            dealtWith.add(secondWord)\n",
    "        else:\n",
    "            if firstWord in keyphrases and firstWord not in dealtWith: \n",
    "                modifiedKeyphrases.add(firstWord)\n",
    "\n",
    "            #if this is the last word in the text, and it is a keyword,\n",
    "            #it definitely has no chance of being a keyphrase at this point    \n",
    "            if j == len(textlist)-1 and secondWord in keyphrases and secondWord not in dealtWith:\n",
    "                modifiedKeyphrases.add(secondWord)\n",
    "        \n",
    "        i = i + 1\n",
    "        j = j + 1\n",
    "        \n",
    "    return modifiedKeyphrases\n",
    "\n",
    "def extractSentences(text):\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentenceTokens = sent_detector.tokenize(text.strip())\n",
    "    graph = buildGraph(sentenceTokens)\n",
    "\n",
    "    calculated_page_rank = nx.pagerank(graph, weight='weight')\n",
    "\n",
    "    #most important sentences in ascending order of importance\n",
    "    sentences = sorted(calculated_page_rank, key=calculated_page_rank.get, reverse=True)\n",
    "\n",
    "    #return a 100 word summary\n",
    "    summary = ' '.join(sentences)\n",
    "    summaryWords = summary.split()\n",
    "    summaryWords = summaryWords[0:101]\n",
    "    summary = ' '.join(summaryWords)\n",
    "\n",
    "    return summary\n",
    "\n",
    "def writeFiles(keyphrases, fileName):\n",
    "    \"outputs the keyphrases and summaries to appropriate files\"\n",
    "    print \"Generating output to \" + 'keywords/' + fileName\n",
    "    keyphraseFile = io.open('keywords/' + fileName, 'wb')\n",
    "    for keyphrase in keyphrases:\n",
    "        #keyphraseFile.write(codecs.BOM_UTF16_LE)\n",
    "        keyphraseFile.write(keyphrase.encode(\"iso-8859-1\") + '\\n')\n",
    "    keyphraseFile.close()\n",
    "\n",
    "    #print \"Generating output to \" + 'summaries/' + fileName\n",
    "    #summaryFile = io.open('summaries/' + fileName, 'w', encoding=\"iso-8859-1\")\n",
    "    #summaryFile.write(summary)\n",
    "    #summaryFile.close()\n",
    "\n",
    "    print \"-\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading articles/1880603.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception KeyboardInterrupt in 'zmq.backend.cython.message.Frame.__dealloc__' ignored\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f0f58ba8b535>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[0marticleFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'articles/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticleFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[0mkeyphrases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextractKeyphrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[0mwriteFiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyphrases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-f0f58ba8b535>\u001b[0m in \u001b[0;36mextractKeyphrases\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;31m#pageRank - initial value of 1.0, error tolerance of 0,0001,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[0mcalculated_page_rank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpagerank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weight'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;31m#most important words in ascending order of importance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\antonio\\Anaconda\\lib\\site-packages\\networkx\\algorithms\\link_analysis\\pagerank_alg.pyc\u001b[0m in \u001b[0;36mpagerank\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\antonio\\Anaconda\\lib\\site-packages\\networkx\\utils\\decorators.pyc\u001b[0m in \u001b[0;36m_not_implemented_for\u001b[1;34m(f, *args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m                                             ' '.join(graph_types))\n\u001b[0;32m     67\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_not_implemented_for\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\antonio\\Anaconda\\lib\\site-packages\\networkx\\algorithms\\link_analysis\\pagerank_alg.pyc\u001b[0m in \u001b[0;36mpagerank\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_directed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_directed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\antonio\\Anaconda\\lib\\site-packages\\networkx\\classes\\graph.pyc\u001b[0m in \u001b[0;36mto_directed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1400\u001b[0m         \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m         G.add_edges_from( ((u,v,deepcopy(data)) \n\u001b[1;32m-> 1402\u001b[1;33m                            \u001b[1;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnbrs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjacency_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1403\u001b[0m                            for v,data in nbrs.items()) )\n\u001b[0;32m   1404\u001b[0m         \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\antonio\\Anaconda\\lib\\site-packages\\networkx\\classes\\digraph.pyc\u001b[0m in \u001b[0;36madd_edges_from\u001b[1;34m(self, ebunch, attr_dict, **attr)\u001b[0m\n\u001b[0;32m    550\u001b[0m                     \"The attr_dict argument must be a dict.\")\n\u001b[0;32m    551\u001b[0m         \u001b[1;31m# process ebunch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mebunch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[0mne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mne\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\antonio\\Anaconda\\lib\\site-packages\\networkx\\classes\\graph.pyc\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m((u, nbrs))\u001b[0m\n\u001b[0;32m   1401\u001b[0m         G.add_edges_from( ((u,v,deepcopy(data)) \n\u001b[0;32m   1402\u001b[0m                            \u001b[1;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnbrs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjacency_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1403\u001b[1;33m                            for v,data in nbrs.items()) )\n\u001b[0m\u001b[0;32m   1404\u001b[0m         \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\antonio\\Anaconda\\lib\\copy.pyc\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\antonio\\Anaconda\\lib\\copy.pyc\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\antonio\\Anaconda\\lib\\copy.pyc\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m     \u001b[0m_keep_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Make sure x lives at least as long as d\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\antonio\\Anaconda\\lib\\copy.pyc\u001b[0m in \u001b[0;36m_keep_alive\u001b[1;34m(x, memo)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \"\"\"\n\u001b[0;32m    277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m         \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;31m# aha, this is the first one :-)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#retrieve each of the articles\n",
    "articles = os.listdir(\"articles\")\n",
    "alain = os.listdir(\"keywords\")\n",
    "for article in articles:\n",
    "    if article not in alain:\n",
    "        print 'Reading articles/' + article\n",
    "        articleFile = io.open('articles/' + article, 'r', encoding=\"iso-8859-1\")\n",
    "        text = articleFile.read()\n",
    "        keyphrases = extractKeyphrases(text)\n",
    "        writeFiles(keyphrases, article)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimal generating sets\n",
      "linear diophantine equations\n",
      "minimal supporting set\n",
      "minimal set\n",
      "linear constraints\n",
      "upper bounds\n",
      "natural numbers\n",
      "nonstrict inequations\n",
      "strict inequations\n",
      "mixed types\n",
      "considered types\n",
      "set\n",
      "types\n",
      "considered\n",
      "constructing\n",
      "solutions\n",
      "solving\n",
      "system\n",
      "compatibility\n",
      "systems\n",
      "criteria\n",
      "construction\n",
      "algorithms\n",
      "components\n"
     ]
    }
   ],
   "source": [
    "# Implementation of RAKE - Rapid Automtic Keyword Exraction algorithm\n",
    "# as described in:\n",
    "# Rose, S., D. Engel, N. Cramer, and W. Cowley (2010). \n",
    "# Automatic keyword extraction from indi-vidual documents. \n",
    "# In M. W. Berry and J. Kogan (Eds.), Text Mining: Applications and Theory.unknown: John Wiley and Sons, Ltd.\n",
    "\n",
    "import re\n",
    "import operator\n",
    "import codecs\n",
    "debug = False\n",
    "test = True\n",
    "\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s) if '.' in s else int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_stop_words(stop_word_file):\n",
    "    \"\"\"\n",
    "    Utility function to load stop words from a file and return as a list of words\n",
    "    @param stop_word_file Path and file name of a file containing stop words.\n",
    "    @return list A list of stop words.\n",
    "    \"\"\"\n",
    "    stop_words = []\n",
    "    for line in open(stop_word_file):\n",
    "        if line.strip()[0:1] != \"#\":\n",
    "            for word in line.split():  # in case more than one per line\n",
    "                stop_words.append(word)\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def separate_words(text, min_word_return_size):\n",
    "    \"\"\"\n",
    "    Utility function to return a list of all words that are have a length greater than a specified number of characters.\n",
    "    @param text The text that must be split in to words.\n",
    "    @param min_word_return_size The minimum no of characters a word must have to be included.\n",
    "    \"\"\"\n",
    "    splitter = re.compile('[^a-zA-Z0-9_\\\\+\\\\-/]')\n",
    "    words = []\n",
    "    for single_word in splitter.split(text):\n",
    "        current_word = single_word.strip().lower()\n",
    "        #leave numbers in phrase, but don't count as words, since they tend to invalidate scores of their phrases\n",
    "        if len(current_word) > min_word_return_size and current_word != '' and not is_number(current_word):\n",
    "            words.append(current_word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    \"\"\"\n",
    "    Utility function to return a list of sentences.\n",
    "    @param text The text that must be split in to sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[.!?,;:\\t\\\\\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013]|\\\\s\\\\-\\\\s')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def build_stop_word_regex(stop_word_file_path):\n",
    "    stop_word_list = load_stop_words(stop_word_file_path)\n",
    "    stop_word_regex_list = []\n",
    "    for word in stop_word_list:\n",
    "        word_regex = r'\\b' + word + r'(?![\\w-])'  # added look ahead for hyphen\n",
    "        stop_word_regex_list.append(word_regex)\n",
    "    stop_word_pattern = re.compile('|'.join(stop_word_regex_list), re.IGNORECASE)\n",
    "    return stop_word_pattern\n",
    "\n",
    "\n",
    "def generate_candidate_keywords(sentence_list, stopword_pattern):\n",
    "    phrase_list = []\n",
    "    for s in sentence_list:\n",
    "        tmp = re.sub(stopword_pattern, '|', s.strip())\n",
    "        phrases = tmp.split(\"|\")\n",
    "        for phrase in phrases:\n",
    "            phrase = phrase.strip().lower()\n",
    "            if phrase != \"\":\n",
    "                phrase_list.append(phrase)\n",
    "    return phrase_list\n",
    "\n",
    "\n",
    "def calculate_word_scores(phraseList):\n",
    "    word_frequency = {}\n",
    "    word_degree = {}\n",
    "    for phrase in phraseList:\n",
    "        word_list = separate_words(phrase, 0)\n",
    "        word_list_length = len(word_list)\n",
    "        word_list_degree = word_list_length - 1\n",
    "        #if word_list_degree > 3: word_list_degree = 3 #exp.\n",
    "        for word in word_list:\n",
    "            word_frequency.setdefault(word, 0)\n",
    "            word_frequency[word] += 1\n",
    "            word_degree.setdefault(word, 0)\n",
    "            word_degree[word] += word_list_degree  #orig.\n",
    "            #word_degree[word] += 1/(word_list_length*1.0) #exp.\n",
    "    for item in word_frequency:\n",
    "        word_degree[item] = word_degree[item] + word_frequency[item]\n",
    "\n",
    "    # Calculate Word scores = deg(w)/frew(w)\n",
    "    word_score = {}\n",
    "    for item in word_frequency:\n",
    "        word_score.setdefault(item, 0)\n",
    "        word_score[item] = word_degree[item] / (word_frequency[item] * 1.0)  #orig.\n",
    "    #word_score[item] = word_frequency[item]/(word_degree[item] * 1.0) #exp.\n",
    "    return word_score\n",
    "\n",
    "\n",
    "def generate_candidate_keyword_scores(phrase_list, word_score):\n",
    "    keyword_candidates = {}\n",
    "    for phrase in phrase_list:\n",
    "        keyword_candidates.setdefault(phrase, 0)\n",
    "        word_list = separate_words(phrase, 0)\n",
    "        candidate_score = 0\n",
    "        for word in word_list:\n",
    "            candidate_score += word_score[word]\n",
    "        keyword_candidates[phrase] = candidate_score\n",
    "    return keyword_candidates\n",
    "\n",
    "\n",
    "class Rake(object):\n",
    "    def __init__(self, stop_words_path):\n",
    "        self.stop_words_path = stop_words_path\n",
    "        self.__stop_words_pattern = build_stop_word_regex(stop_words_path)\n",
    "\n",
    "    def run(self, text):\n",
    "        sentence_list = split_sentences(text)\n",
    "\n",
    "        phrase_list = generate_candidate_keywords(sentence_list, self.__stop_words_pattern)\n",
    "\n",
    "        word_scores = calculate_word_scores(phrase_list)\n",
    "\n",
    "        keyword_candidates = generate_candidate_keyword_scores(phrase_list, word_scores)\n",
    "\n",
    "        sorted_keywords = sorted(keyword_candidates.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "        return sorted_keywords\n",
    "\n",
    "\n",
    "if test:\n",
    "    text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types of systems and systems of mixed types.\"\n",
    "\n",
    "    # Split text into sentences\n",
    "    sentenceList = split_sentences(text)\n",
    "    #stoppath = \"FoxStoplist.txt\" #Fox stoplist contains \"numbers\", so it will not find \"natural numbers\" like in Table 1.1\n",
    "    stoppath = \"SmartStoplist.txt\"  #SMART stoplist misses some of the lower-scoring keywords in Figure 1.5, which means that the top 1/3 cuts off one of the 4.0 score words in Table 1.1\n",
    "    stopwordpattern = build_stop_word_regex(stoppath)\n",
    "\n",
    "    # generate candidate keywords\n",
    "    phraseList = generate_candidate_keywords(sentenceList, stopwordpattern)\n",
    "\n",
    "    # calculate individual word scores\n",
    "    wordscores = calculate_word_scores(phraseList)\n",
    "\n",
    "    # generate candidate keyword scores\n",
    "    keywordcandidates = generate_candidate_keyword_scores(phraseList, wordscores)\n",
    "    if debug: print keywordcandidates\n",
    "\n",
    "    sortedKeywords = sorted(keywordcandidates.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    if debug: print sortedKeywords\n",
    "\n",
    "    totalKeywords = len(sortedKeywords)\n",
    "    if debug: print totalKeywords\n",
    "    #print sortedKeywords[0:(totalKeywords / 3)]\n",
    "\n",
    "    rake = Rake(\"SmartStoplist.txt\")\n",
    "    keywords = rake.run(text)\n",
    "    #print keywords\n",
    "    for i in keywords:\n",
    "        print i[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading articles/100166.txt\n",
      "Generating output to keywords/100166.txt\n",
      "Reading articles/101.txt\n",
      "Generating output to keywords/101.txt\n",
      "Reading articles/1016800.txt\n",
      "Generating output to keywords/1016800.txt\n",
      "Reading articles/101973.txt\n",
      "Generating output to keywords/101973.txt\n",
      "Reading articles/1036681.txt\n",
      "Generating output to keywords/1036681.txt\n",
      "Reading articles/106364.txt\n",
      "Generating output to keywords/106364.txt\n",
      "Reading articles/1067802.txt\n",
      "Generating output to keywords/1067802.txt\n",
      "Reading articles/1097231.txt\n",
      "Generating output to keywords/1097231.txt\n",
      "Reading articles/1116998.txt\n",
      "Generating output to keywords/1116998.txt\n",
      "Reading articles/112878.txt\n",
      "Generating output to keywords/112878.txt\n",
      "Reading articles/1133633.txt\n",
      "Generating output to keywords/1133633.txt\n",
      "Reading articles/1137519.txt\n",
      "Generating output to keywords/1137519.txt\n",
      "Reading articles/114199.txt\n",
      "Generating output to keywords/114199.txt\n",
      "Reading articles/1144477.txt\n",
      "Generating output to keywords/1144477.txt\n",
      "Reading articles/1159615.txt\n",
      "Generating output to keywords/1159615.txt\n",
      "Reading articles/118649.txt\n",
      "Generating output to keywords/118649.txt\n",
      "Reading articles/118727.txt\n",
      "Generating output to keywords/118727.txt\n",
      "Reading articles/118744.txt\n",
      "Generating output to keywords/118744.txt\n",
      "Reading articles/1197981.txt\n",
      "Generating output to keywords/1197981.txt\n",
      "Reading articles/1202726.txt\n",
      "Generating output to keywords/1202726.txt\n",
      "Reading articles/1206611.txt\n",
      "Generating output to keywords/1206611.txt\n",
      "Reading articles/121661.txt\n",
      "Generating output to keywords/121661.txt\n",
      "Reading articles/1226851.txt\n",
      "Generating output to keywords/1226851.txt\n",
      "Reading articles/126997.txt\n",
      "Generating output to keywords/126997.txt\n",
      "Reading articles/1272477.txt\n",
      "Generating output to keywords/1272477.txt\n",
      "Reading articles/1272533.txt\n",
      "Generating output to keywords/1272533.txt\n",
      "Reading articles/1307464.txt\n",
      "Generating output to keywords/1307464.txt\n",
      "Reading articles/1320727.txt\n",
      "Generating output to keywords/1320727.txt\n",
      "Reading articles/1322799.txt\n",
      "Generating output to keywords/1322799.txt\n",
      "Reading articles/1322886.txt\n",
      "Generating output to keywords/1322886.txt\n",
      "Reading articles/1336057.txt\n",
      "Generating output to keywords/1336057.txt\n",
      "Reading articles/1362387.txt\n",
      "Generating output to keywords/1362387.txt\n",
      "Reading articles/136657.txt\n",
      "Generating output to keywords/136657.txt\n",
      "Reading articles/137111.txt\n",
      "Generating output to keywords/137111.txt\n",
      "Reading articles/1388250.txt\n",
      "Generating output to keywords/1388250.txt\n",
      "Reading articles/1391621.txt\n",
      "Generating output to keywords/1391621.txt\n",
      "Reading articles/1392584.txt\n",
      "Generating output to keywords/1392584.txt\n",
      "Reading articles/1392792.txt\n",
      "Generating output to keywords/1392792.txt\n",
      "Reading articles/141524.txt\n",
      "Generating output to keywords/141524.txt\n",
      "Reading articles/141840.txt\n",
      "Generating output to keywords/141840.txt\n",
      "Reading articles/1418865.txt\n",
      "Generating output to keywords/1418865.txt\n",
      "Reading articles/142323.txt\n",
      "Generating output to keywords/142323.txt\n",
      "Reading articles/142488.txt\n",
      "Generating output to keywords/142488.txt\n",
      "Reading articles/1453145.txt\n",
      "Generating output to keywords/1453145.txt\n",
      "Reading articles/1465869.txt\n",
      "Generating output to keywords/1465869.txt\n",
      "Reading articles/150261.txt\n",
      "Generating output to keywords/150261.txt\n",
      "Reading articles/1532668.txt\n",
      "Generating output to keywords/1532668.txt\n",
      "Reading articles/1551105.txt\n",
      "Generating output to keywords/1551105.txt\n",
      "Reading articles/155900.txt\n",
      "Generating output to keywords/155900.txt\n",
      "Reading articles/1568644.txt\n",
      "Generating output to keywords/1568644.txt\n",
      "Reading articles/1602005.txt\n",
      "Generating output to keywords/1602005.txt\n",
      "Reading articles/1610049.txt\n",
      "Generating output to keywords/1610049.txt\n",
      "Reading articles/1610369.txt\n",
      "Generating output to keywords/1610369.txt\n",
      "Reading articles/1624776.txt\n",
      "Generating output to keywords/1624776.txt\n",
      "Reading articles/1631613.txt\n",
      "Generating output to keywords/1631613.txt\n",
      "Reading articles/1632947.txt\n",
      "Generating output to keywords/1632947.txt\n",
      "Reading articles/1752368.txt\n",
      "Generating output to keywords/1752368.txt\n",
      "Reading articles/1794647.txt\n",
      "Generating output to keywords/1794647.txt\n",
      "Reading articles/1880339.txt\n",
      "Generating output to keywords/1880339.txt\n",
      "Reading articles/1880603.txt\n",
      "Generating output to keywords/1880603.txt\n",
      "Reading articles/1910555.txt\n",
      "Generating output to keywords/1910555.txt\n",
      "Reading articles/1926414.txt\n",
      "Generating output to keywords/1926414.txt\n",
      "Reading articles/1989097.txt\n",
      "Generating output to keywords/1989097.txt\n",
      "Reading articles/2100.txt\n",
      "Generating output to keywords/2100.txt\n",
      "Reading articles/2107.txt\n",
      "Generating output to keywords/2107.txt\n",
      "Reading articles/211497.txt\n",
      "Generating output to keywords/211497.txt\n",
      "Reading articles/2163327.txt\n",
      "Generating output to keywords/2163327.txt\n",
      "Reading articles/2235507.txt\n",
      "Generating output to keywords/2235507.txt\n",
      "Reading articles/226992.txt\n",
      "Generating output to keywords/226992.txt\n",
      "Reading articles/227153.txt\n",
      "Generating output to keywords/227153.txt\n",
      "Reading articles/227174.txt\n",
      "Generating output to keywords/227174.txt\n",
      "Reading articles/2288308.txt\n",
      "Generating output to keywords/2288308.txt\n",
      "Reading articles/229.txt\n",
      "Generating output to keywords/229.txt\n",
      "Reading articles/231294.txt\n",
      "Generating output to keywords/231294.txt\n",
      "Reading articles/238188.txt\n",
      "Generating output to keywords/238188.txt\n",
      "Reading articles/239528.txt\n",
      "Generating output to keywords/239528.txt\n",
      "Reading articles/239569.txt\n",
      "Generating output to keywords/239569.txt\n",
      "Reading articles/239581.txt\n",
      "Generating output to keywords/239581.txt\n",
      "Reading articles/241030.txt\n",
      "Generating output to keywords/241030.txt\n",
      "Reading articles/248.txt\n",
      "Generating output to keywords/248.txt\n",
      "Reading articles/249.txt\n",
      "Generating output to keywords/249.txt\n",
      "Reading articles/261639.txt\n",
      "Generating output to keywords/261639.txt\n",
      "Reading articles/265789.txt\n",
      "Generating output to keywords/265789.txt\n",
      "Reading articles/272.txt\n",
      "Generating output to keywords/272.txt\n",
      "Reading articles/272363.txt\n",
      "Generating output to keywords/272363.txt\n",
      "Reading articles/292.txt\n",
      "Generating output to keywords/292.txt\n",
      "Reading articles/302050.txt\n",
      "Generating output to keywords/302050.txt\n",
      "Reading articles/303213.txt\n",
      "Generating output to keywords/303213.txt\n",
      "Reading articles/303889.txt\n",
      "Generating output to keywords/303889.txt\n",
      "Reading articles/307461.txt\n",
      "Generating output to keywords/307461.txt\n",
      "Reading articles/309778.txt\n",
      "Generating output to keywords/309778.txt\n",
      "Reading articles/312119.txt\n",
      "Generating output to keywords/312119.txt\n",
      "Reading articles/312124.txt\n",
      "Generating output to keywords/312124.txt\n",
      "Reading articles/312476.txt\n",
      "Generating output to keywords/312476.txt\n",
      "Reading articles/329170.txt\n",
      "Generating output to keywords/329170.txt\n",
      "Reading articles/332150.txt\n",
      "Generating output to keywords/332150.txt\n",
      "Reading articles/332173.txt\n",
      "Generating output to keywords/332173.txt\n",
      "Reading articles/333353.txt\n",
      "Generating output to keywords/333353.txt\n",
      "Reading articles/334264.txt\n",
      "Generating output to keywords/334264.txt\n",
      "Reading articles/341252.txt\n",
      "Generating output to keywords/341252.txt\n",
      "Reading articles/353537.txt\n",
      "Generating output to keywords/353537.txt\n",
      "Reading articles/353538.txt\n",
      "Generating output to keywords/353538.txt\n",
      "Reading articles/354027.txt\n",
      "Generating output to keywords/354027.txt\n",
      "Reading articles/355573.txt\n",
      "Generating output to keywords/355573.txt\n",
      "Reading articles/355574.txt\n",
      "Generating output to keywords/355574.txt\n",
      "Reading articles/3594.txt\n",
      "Generating output to keywords/3594.txt\n",
      "Reading articles/363614.txt\n",
      "Generating output to keywords/363614.txt\n",
      "Reading articles/375823.txt\n",
      "Generating output to keywords/375823.txt\n",
      "Reading articles/400238.txt\n",
      "Generating output to keywords/400238.txt\n",
      "Reading articles/406519.txt\n",
      "Generating output to keywords/406519.txt\n",
      "Reading articles/407273.txt\n",
      "Generating output to keywords/407273.txt\n",
      "Reading articles/415502.txt\n",
      "Generating output to keywords/415502.txt\n",
      "Reading articles/420465.txt\n",
      "Generating output to keywords/420465.txt\n",
      "Reading articles/422950.txt\n",
      "Generating output to keywords/422950.txt\n",
      "Reading articles/43.txt\n",
      "Generating output to keywords/43.txt\n",
      "Reading articles/430079.txt\n",
      "Generating output to keywords/430079.txt\n",
      "Reading articles/437770.txt\n",
      "Generating output to keywords/437770.txt\n",
      "Reading articles/438129.txt\n",
      "Generating output to keywords/438129.txt\n",
      "Reading articles/44.txt\n",
      "Generating output to keywords/44.txt\n",
      "Reading articles/444860.txt\n",
      "Generating output to keywords/444860.txt\n",
      "Reading articles/446839.txt\n",
      "Generating output to keywords/446839.txt\n",
      "Reading articles/454555.txt\n",
      "Generating output to keywords/454555.txt\n",
      "Reading articles/459365.txt\n",
      "Generating output to keywords/459365.txt\n",
      "Reading articles/460153.txt\n",
      "Generating output to keywords/460153.txt\n",
      "Reading articles/465989.txt\n",
      "Generating output to keywords/465989.txt\n",
      "Reading articles/466050.txt\n",
      "Generating output to keywords/466050.txt\n",
      "Reading articles/466068.txt\n",
      "Generating output to keywords/466068.txt\n",
      "Reading articles/47.txt\n",
      "Generating output to keywords/47.txt\n",
      "Reading articles/477450.txt\n",
      "Generating output to keywords/477450.txt\n",
      "Reading articles/478707.txt\n",
      "Generating output to keywords/478707.txt\n",
      "Reading articles/482101.txt\n",
      "Generating output to keywords/482101.txt\n",
      "Reading articles/499796.txt\n",
      "Generating output to keywords/499796.txt\n",
      "Reading articles/504894.txt\n",
      "Generating output to keywords/504894.txt\n",
      "Reading articles/506455.txt\n",
      "Generating output to keywords/506455.txt\n",
      "Reading articles/506468.txt\n",
      "Generating output to keywords/506468.txt\n",
      "Reading articles/507525.txt\n",
      "Generating output to keywords/507525.txt\n",
      "Reading articles/507926.txt\n",
      "Generating output to keywords/507926.txt\n",
      "Reading articles/509425.txt\n",
      "Generating output to keywords/509425.txt\n",
      "Reading articles/516580.txt\n",
      "Generating output to keywords/516580.txt\n",
      "Reading articles/523878.txt\n",
      "Generating output to keywords/523878.txt\n",
      "Reading articles/525366.txt\n",
      "Generating output to keywords/525366.txt\n",
      "Reading articles/528160.txt\n",
      "Generating output to keywords/528160.txt\n",
      "Reading articles/540889.txt\n",
      "Generating output to keywords/540889.txt\n",
      "Reading articles/546157.txt\n",
      "Generating output to keywords/546157.txt\n",
      "Reading articles/549806.txt\n",
      "Generating output to keywords/549806.txt\n",
      "Reading articles/553494.txt\n",
      "Generating output to keywords/553494.txt\n",
      "Reading articles/553497.txt\n",
      "Generating output to keywords/553497.txt\n",
      "Reading articles/559064.txt\n",
      "Generating output to keywords/559064.txt\n",
      "Reading articles/560813.txt\n",
      "Generating output to keywords/560813.txt\n",
      "Reading articles/571538.txt\n",
      "Generating output to keywords/571538.txt\n",
      "Reading articles/602903.txt\n",
      "Generating output to keywords/602903.txt\n",
      "Reading articles/609199.txt\n",
      "Generating output to keywords/609199.txt\n",
      "Reading articles/612608.txt\n",
      "Generating output to keywords/612608.txt\n",
      "Reading articles/613191.txt\n",
      "Generating output to keywords/613191.txt\n",
      "Reading articles/620656.txt\n",
      "Generating output to keywords/620656.txt\n",
      "Reading articles/622483.txt\n",
      "Generating output to keywords/622483.txt\n",
      "Reading articles/656280.txt\n",
      "Generating output to keywords/656280.txt\n",
      "Reading articles/668899.txt\n",
      "Generating output to keywords/668899.txt\n",
      "Reading articles/668933.txt\n",
      "Generating output to keywords/668933.txt\n",
      "Reading articles/698675.txt\n",
      "Generating output to keywords/698675.txt\n",
      "Reading articles/702349.txt\n",
      "Generating output to keywords/702349.txt\n",
      "Reading articles/738207.txt\n",
      "Generating output to keywords/738207.txt\n",
      "Reading articles/740681.txt\n",
      "Generating output to keywords/740681.txt\n",
      "Reading articles/771131.txt\n",
      "Generating output to keywords/771131.txt\n",
      "Reading articles/771144.txt\n",
      "Generating output to keywords/771144.txt\n",
      "Reading articles/771168.txt\n",
      "Generating output to keywords/771168.txt\n",
      "Reading articles/778023.txt\n",
      "Generating output to keywords/778023.txt\n",
      "Reading articles/802975.txt\n",
      "Generating output to keywords/802975.txt\n",
      "Reading articles/80410.txt\n",
      "Generating output to keywords/80410.txt\n",
      "Reading articles/81501.txt\n",
      "Generating output to keywords/81501.txt\n",
      "Reading articles/833421.txt\n",
      "Generating output to keywords/833421.txt\n",
      "Reading articles/84176.txt\n",
      "Generating output to keywords/84176.txt\n",
      "Reading articles/84309.txt\n",
      "Generating output to keywords/84309.txt\n",
      "Reading articles/86487.txt\n",
      "Generating output to keywords/86487.txt\n",
      "Reading articles/86865.txt\n",
      "Generating output to keywords/86865.txt\n",
      "Reading articles/880918.txt\n",
      "Generating output to keywords/880918.txt\n",
      "Reading articles/904109.txt\n",
      "Generating output to keywords/904109.txt\n",
      "Reading articles/90558.txt\n",
      "Generating output to keywords/90558.txt\n",
      "Reading articles/937059.txt\n",
      "Generating output to keywords/937059.txt\n",
      "Reading articles/957831.txt\n",
      "Generating output to keywords/957831.txt\n",
      "Reading articles/98486.txt\n",
      "Generating output to keywords/98486.txt\n",
      "Reading articles/99.txt\n",
      "Generating output to keywords/99.txt\n",
      "Reading articles/99033.txt\n",
      "Generating output to keywords/99033.txt\n"
     ]
    }
   ],
   "source": [
    "articles = os.listdir(\"articles\")\n",
    "for article in articles:\n",
    "    print 'Reading articles/' + article\n",
    "    articleFile = io.open('articles/' + article, 'r', encoding=\"iso-8859-1\")\n",
    "    text = articleFile.read()\n",
    "   # Split text into sentences\n",
    "    sentenceList = split_sentences(text)\n",
    "    #stoppath = \"FoxStoplist.txt\" #Fox stoplist contains \"numbers\", so it will not find \"natural numbers\" like in Table 1.1\n",
    "    stoppath = \"SmartStoplist.txt\"  #SMART stoplist misses some of the lower-scoring keywords in Figure 1.5, which means that the top 1/3 cuts off one of the 4.0 score words in Table 1.1\n",
    "    stopwordpattern = build_stop_word_regex(stoppath)\n",
    "\n",
    "    # generate candidate keywords\n",
    "    phraseList = generate_candidate_keywords(sentenceList, stopwordpattern)\n",
    "\n",
    "    # calculate individual word scores\n",
    "    wordscores = calculate_word_scores(phraseList)\n",
    "\n",
    "    # generate candidate keyword scores\n",
    "    keywordcandidates = generate_candidate_keyword_scores(phraseList, wordscores)\n",
    "    if debug: print keywordcandidates\n",
    "\n",
    "    sortedKeywords = sorted(keywordcandidates.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    if debug: print sortedKeywords\n",
    "\n",
    "    totalKeywords = len(sortedKeywords)\n",
    "    if debug: print totalKeywords\n",
    "    #print sortedKeywords[0:(totalKeywords / 3)]\n",
    "\n",
    "    rake = Rake(\"SmartStoplist.txt\")\n",
    "    keywords = rake.run(text)\n",
    "    #print keywords\n",
    "    \n",
    "    \"outputs the keyphrases and summaries to appropriate files\"\n",
    "    print \"Generating output to \" + 'keywords/' + article\n",
    "    keyphraseFile = io.open('keywordsRAKE/' + article, 'wb')\n",
    "    for keywords in keywords:\n",
    "        #keyphraseFile.write(codecs.BOM_UTF16_LE)\n",
    "        keyphraseFile.write(keywords[0].encode(\"iso-8859-1\") + '\\n')\n",
    "    keyphraseFile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from newspaper import Article, fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading articles/100166.txt\n",
      "100166.txt\n",
      "Generating output to keywords/100166.txt\n",
      "Reading articles/101.txt\n",
      "101.txt\n",
      "Generating output to keywords/101.txt\n",
      "Reading articles/1016800.txt\n",
      "1016800.txt\n",
      "Generating output to keywords/1016800.txt\n",
      "Reading articles/101973.txt\n",
      "101973.txt\n",
      "Generating output to keywords/101973.txt\n",
      "Reading articles/1036681.txt\n",
      "1036681.txt\n",
      "Generating output to keywords/1036681.txt\n",
      "Reading articles/106364.txt\n",
      "106364.txt\n",
      "Generating output to keywords/106364.txt\n",
      "Reading articles/1067802.txt\n",
      "1067802.txt\n",
      "Generating output to keywords/1067802.txt\n",
      "Reading articles/1097231.txt\n",
      "1097231.txt\n",
      "Generating output to keywords/1097231.txt\n",
      "Reading articles/1116998.txt\n",
      "1116998.txt\n",
      "Generating output to keywords/1116998.txt\n",
      "Reading articles/112878.txt\n",
      "112878.txt\n",
      "Generating output to keywords/112878.txt\n",
      "Reading articles/1133633.txt\n",
      "1133633.txt\n",
      "Generating output to keywords/1133633.txt\n",
      "Reading articles/1137519.txt\n",
      "1137519.txt\n",
      "Generating output to keywords/1137519.txt\n",
      "Reading articles/114199.txt\n",
      "114199.txt\n",
      "Generating output to keywords/114199.txt\n",
      "Reading articles/1144477.txt\n",
      "1144477.txt\n",
      "Generating output to keywords/1144477.txt\n",
      "Reading articles/1159615.txt\n",
      "1159615.txt\n",
      "Generating output to keywords/1159615.txt\n",
      "Reading articles/118649.txt\n",
      "118649.txt\n",
      "Generating output to keywords/118649.txt\n",
      "Reading articles/118727.txt\n",
      "118727.txt\n",
      "Generating output to keywords/118727.txt\n",
      "Reading articles/118744.txt\n",
      "118744.txt\n",
      "Generating output to keywords/118744.txt\n",
      "Reading articles/1197981.txt\n",
      "1197981.txt\n",
      "Generating output to keywords/1197981.txt\n",
      "Reading articles/1202726.txt\n",
      "1202726.txt\n",
      "Generating output to keywords/1202726.txt\n",
      "Reading articles/1206611.txt\n",
      "1206611.txt\n",
      "Generating output to keywords/1206611.txt\n",
      "Reading articles/121661.txt\n",
      "121661.txt\n",
      "Generating output to keywords/121661.txt\n",
      "Reading articles/1226851.txt\n",
      "1226851.txt\n",
      "Generating output to keywords/1226851.txt\n",
      "Reading articles/126997.txt\n",
      "126997.txt\n",
      "Generating output to keywords/126997.txt\n",
      "Reading articles/1272477.txt\n",
      "1272477.txt\n",
      "Generating output to keywords/1272477.txt\n",
      "Reading articles/1272533.txt\n",
      "1272533.txt\n",
      "Generating output to keywords/1272533.txt\n",
      "Reading articles/1307464.txt\n",
      "1307464.txt\n",
      "Generating output to keywords/1307464.txt\n",
      "Reading articles/1320727.txt\n",
      "1320727.txt\n",
      "Generating output to keywords/1320727.txt\n",
      "Reading articles/1322799.txt\n",
      "1322799.txt\n",
      "Generating output to keywords/1322799.txt\n",
      "Reading articles/1322886.txt\n",
      "1322886.txt\n",
      "Generating output to keywords/1322886.txt\n",
      "Reading articles/1336057.txt\n",
      "1336057.txt\n",
      "Generating output to keywords/1336057.txt\n",
      "Reading articles/1362387.txt\n",
      "1362387.txt\n",
      "Generating output to keywords/1362387.txt\n",
      "Reading articles/136657.txt\n",
      "136657.txt\n",
      "Generating output to keywords/136657.txt\n",
      "Reading articles/137111.txt\n",
      "137111.txt\n",
      "Generating output to keywords/137111.txt\n",
      "Reading articles/1388250.txt\n",
      "1388250.txt\n",
      "Generating output to keywords/1388250.txt\n",
      "Reading articles/1391621.txt\n",
      "1391621.txt\n",
      "Generating output to keywords/1391621.txt\n",
      "Reading articles/1392584.txt\n",
      "1392584.txt\n",
      "Generating output to keywords/1392584.txt\n",
      "Reading articles/1392792.txt\n",
      "1392792.txt\n",
      "Generating output to keywords/1392792.txt\n",
      "Reading articles/141524.txt\n",
      "141524.txt\n",
      "Generating output to keywords/141524.txt\n",
      "Reading articles/141840.txt\n",
      "141840.txt\n",
      "Generating output to keywords/141840.txt\n",
      "Reading articles/1418865.txt\n",
      "1418865.txt\n",
      "Generating output to keywords/1418865.txt\n",
      "Reading articles/142323.txt\n",
      "142323.txt\n",
      "Generating output to keywords/142323.txt\n",
      "Reading articles/142488.txt\n",
      "142488.txt\n",
      "Generating output to keywords/142488.txt\n",
      "Reading articles/1453145.txt\n",
      "1453145.txt\n",
      "Generating output to keywords/1453145.txt\n",
      "Reading articles/1465869.txt\n",
      "1465869.txt\n",
      "Generating output to keywords/1465869.txt\n",
      "Reading articles/150261.txt\n",
      "150261.txt\n",
      "Generating output to keywords/150261.txt\n",
      "Reading articles/1532668.txt\n",
      "1532668.txt\n",
      "Generating output to keywords/1532668.txt\n",
      "Reading articles/1551105.txt\n",
      "1551105.txt\n",
      "Generating output to keywords/1551105.txt\n",
      "Reading articles/155900.txt\n",
      "155900.txt\n",
      "Generating output to keywords/155900.txt\n",
      "Reading articles/1568644.txt\n",
      "1568644.txt\n",
      "Generating output to keywords/1568644.txt\n",
      "Reading articles/1602005.txt\n",
      "1602005.txt\n",
      "Generating output to keywords/1602005.txt\n",
      "Reading articles/1610049.txt\n",
      "1610049.txt\n",
      "Generating output to keywords/1610049.txt\n",
      "Reading articles/1610369.txt\n",
      "1610369.txt\n",
      "Generating output to keywords/1610369.txt\n",
      "Reading articles/1624776.txt\n",
      "1624776.txt\n",
      "Generating output to keywords/1624776.txt\n",
      "Reading articles/1631613.txt\n",
      "1631613.txt\n",
      "Generating output to keywords/1631613.txt\n",
      "Reading articles/1632947.txt\n",
      "1632947.txt\n",
      "Generating output to keywords/1632947.txt\n",
      "Reading articles/1752368.txt\n",
      "1752368.txt\n",
      "Generating output to keywords/1752368.txt\n",
      "Reading articles/1794647.txt\n",
      "1794647.txt\n",
      "Generating output to keywords/1794647.txt\n",
      "Reading articles/1880339.txt\n",
      "1880339.txt\n",
      "Generating output to keywords/1880339.txt\n",
      "Reading articles/1880603.txt\n",
      "1880603.txt\n",
      "Generating output to keywords/1880603.txt\n",
      "Reading articles/1910555.txt\n",
      "1910555.txt\n",
      "Generating output to keywords/1910555.txt\n",
      "Reading articles/1926414.txt\n",
      "1926414.txt\n",
      "Generating output to keywords/1926414.txt\n",
      "Reading articles/1989097.txt\n",
      "1989097.txt\n",
      "Generating output to keywords/1989097.txt\n",
      "Reading articles/2100.txt\n",
      "2100.txt\n",
      "Generating output to keywords/2100.txt\n",
      "Reading articles/2107.txt\n",
      "2107.txt\n",
      "Generating output to keywords/2107.txt\n",
      "Reading articles/211497.txt\n",
      "211497.txt\n",
      "Generating output to keywords/211497.txt\n",
      "Reading articles/2163327.txt\n",
      "2163327.txt\n",
      "Generating output to keywords/2163327.txt\n",
      "Reading articles/2235507.txt\n",
      "2235507.txt\n",
      "Generating output to keywords/2235507.txt\n",
      "Reading articles/226992.txt\n",
      "226992.txt\n",
      "Generating output to keywords/226992.txt\n",
      "Reading articles/227153.txt\n",
      "227153.txt\n",
      "Generating output to keywords/227153.txt\n",
      "Reading articles/227174.txt\n",
      "227174.txt\n",
      "Generating output to keywords/227174.txt\n",
      "Reading articles/2288308.txt\n",
      "2288308.txt\n",
      "Generating output to keywords/2288308.txt\n",
      "Reading articles/229.txt\n",
      "229.txt\n",
      "Generating output to keywords/229.txt\n",
      "Reading articles/231294.txt\n",
      "231294.txt\n",
      "Generating output to keywords/231294.txt\n",
      "Reading articles/238188.txt\n",
      "238188.txt\n",
      "Generating output to keywords/238188.txt\n",
      "Reading articles/239528.txt\n",
      "239528.txt\n",
      "Generating output to keywords/239528.txt\n",
      "Reading articles/239569.txt\n",
      "239569.txt\n",
      "Generating output to keywords/239569.txt\n",
      "Reading articles/239581.txt\n",
      "239581.txt\n",
      "Generating output to keywords/239581.txt\n",
      "Reading articles/241030.txt\n",
      "241030.txt\n",
      "Generating output to keywords/241030.txt\n",
      "Reading articles/248.txt\n",
      "248.txt\n",
      "Generating output to keywords/248.txt\n",
      "Reading articles/249.txt\n",
      "249.txt\n",
      "Generating output to keywords/249.txt\n",
      "Reading articles/261639.txt\n",
      "261639.txt\n",
      "Generating output to keywords/261639.txt\n",
      "Reading articles/265789.txt\n",
      "265789.txt\n",
      "Generating output to keywords/265789.txt\n",
      "Reading articles/272.txt\n",
      "272.txt\n",
      "Generating output to keywords/272.txt\n",
      "Reading articles/272363.txt\n",
      "272363.txt\n",
      "Generating output to keywords/272363.txt\n",
      "Reading articles/292.txt\n",
      "292.txt\n",
      "Generating output to keywords/292.txt\n",
      "Reading articles/302050.txt\n",
      "302050.txt\n",
      "Generating output to keywords/302050.txt\n",
      "Reading articles/303213.txt\n",
      "303213.txt\n",
      "Generating output to keywords/303213.txt\n",
      "Reading articles/303889.txt\n",
      "303889.txt\n",
      "Generating output to keywords/303889.txt\n",
      "Reading articles/307461.txt\n",
      "307461.txt\n",
      "Generating output to keywords/307461.txt\n",
      "Reading articles/309778.txt\n",
      "309778.txt\n",
      "Generating output to keywords/309778.txt\n",
      "Reading articles/312119.txt\n",
      "312119.txt\n",
      "Generating output to keywords/312119.txt\n",
      "Reading articles/312124.txt\n",
      "312124.txt\n",
      "Generating output to keywords/312124.txt\n",
      "Reading articles/312476.txt\n",
      "312476.txt\n",
      "Generating output to keywords/312476.txt\n",
      "Reading articles/329170.txt\n",
      "329170.txt\n",
      "Generating output to keywords/329170.txt\n",
      "Reading articles/332150.txt\n",
      "332150.txt\n",
      "Generating output to keywords/332150.txt\n",
      "Reading articles/332173.txt\n",
      "332173.txt\n",
      "Generating output to keywords/332173.txt\n",
      "Reading articles/333353.txt\n",
      "333353.txt\n",
      "Generating output to keywords/333353.txt\n",
      "Reading articles/334264.txt\n",
      "334264.txt\n",
      "Generating output to keywords/334264.txt\n",
      "Reading articles/341252.txt\n",
      "341252.txt\n",
      "Generating output to keywords/341252.txt\n",
      "Reading articles/353537.txt\n",
      "353537.txt\n",
      "Generating output to keywords/353537.txt\n",
      "Reading articles/353538.txt\n",
      "353538.txt\n",
      "Generating output to keywords/353538.txt\n",
      "Reading articles/354027.txt\n",
      "354027.txt\n",
      "Generating output to keywords/354027.txt\n",
      "Reading articles/355573.txt\n",
      "355573.txt\n",
      "Generating output to keywords/355573.txt\n",
      "Reading articles/355574.txt\n",
      "355574.txt\n",
      "Generating output to keywords/355574.txt\n",
      "Reading articles/3594.txt\n",
      "3594.txt\n",
      "Generating output to keywords/3594.txt\n",
      "Reading articles/363614.txt\n",
      "363614.txt\n",
      "Generating output to keywords/363614.txt\n",
      "Reading articles/375823.txt\n",
      "375823.txt\n",
      "Generating output to keywords/375823.txt\n",
      "Reading articles/400238.txt\n",
      "400238.txt\n",
      "Generating output to keywords/400238.txt\n",
      "Reading articles/406519.txt\n",
      "406519.txt\n",
      "Generating output to keywords/406519.txt\n",
      "Reading articles/407273.txt\n",
      "407273.txt\n",
      "Generating output to keywords/407273.txt\n",
      "Reading articles/415502.txt\n",
      "415502.txt\n",
      "Generating output to keywords/415502.txt\n",
      "Reading articles/420465.txt\n",
      "420465.txt\n",
      "Generating output to keywords/420465.txt\n",
      "Reading articles/422950.txt\n",
      "422950.txt\n",
      "Generating output to keywords/422950.txt\n",
      "Reading articles/43.txt\n",
      "43.txt\n",
      "Generating output to keywords/43.txt\n",
      "Reading articles/430079.txt\n",
      "430079.txt\n",
      "Generating output to keywords/430079.txt\n",
      "Reading articles/437770.txt\n",
      "437770.txt\n",
      "Generating output to keywords/437770.txt\n",
      "Reading articles/438129.txt\n",
      "438129.txt\n",
      "Generating output to keywords/438129.txt\n",
      "Reading articles/44.txt\n",
      "44.txt\n",
      "Generating output to keywords/44.txt\n",
      "Reading articles/444860.txt\n",
      "444860.txt\n",
      "Generating output to keywords/444860.txt\n",
      "Reading articles/446839.txt\n",
      "446839.txt\n",
      "Generating output to keywords/446839.txt\n",
      "Reading articles/454555.txt\n",
      "454555.txt\n",
      "Generating output to keywords/454555.txt\n",
      "Reading articles/459365.txt\n",
      "459365.txt\n",
      "Generating output to keywords/459365.txt\n",
      "Reading articles/460153.txt\n",
      "460153.txt\n",
      "Generating output to keywords/460153.txt\n",
      "Reading articles/465989.txt\n",
      "465989.txt\n",
      "Generating output to keywords/465989.txt\n",
      "Reading articles/466050.txt\n",
      "466050.txt\n",
      "Generating output to keywords/466050.txt\n",
      "Reading articles/466068.txt\n",
      "466068.txt\n",
      "Generating output to keywords/466068.txt\n",
      "Reading articles/47.txt\n",
      "47.txt\n",
      "Generating output to keywords/47.txt\n",
      "Reading articles/477450.txt\n",
      "477450.txt\n",
      "Generating output to keywords/477450.txt\n",
      "Reading articles/478707.txt\n",
      "478707.txt\n",
      "Generating output to keywords/478707.txt\n",
      "Reading articles/482101.txt\n",
      "482101.txt\n",
      "Generating output to keywords/482101.txt\n",
      "Reading articles/499796.txt\n",
      "499796.txt\n",
      "Generating output to keywords/499796.txt\n",
      "Reading articles/504894.txt\n",
      "504894.txt\n",
      "Generating output to keywords/504894.txt\n",
      "Reading articles/506455.txt\n",
      "506455.txt\n",
      "Generating output to keywords/506455.txt\n",
      "Reading articles/506468.txt\n",
      "506468.txt\n",
      "Generating output to keywords/506468.txt\n",
      "Reading articles/507525.txt\n",
      "507525.txt\n",
      "Generating output to keywords/507525.txt\n",
      "Reading articles/507926.txt\n",
      "507926.txt\n",
      "Generating output to keywords/507926.txt\n",
      "Reading articles/509425.txt\n",
      "509425.txt\n",
      "Generating output to keywords/509425.txt\n",
      "Reading articles/516580.txt\n",
      "516580.txt\n",
      "Generating output to keywords/516580.txt\n",
      "Reading articles/523878.txt\n",
      "523878.txt\n",
      "Generating output to keywords/523878.txt\n",
      "Reading articles/525366.txt\n",
      "525366.txt\n",
      "Generating output to keywords/525366.txt\n",
      "Reading articles/528160.txt\n",
      "528160.txt\n",
      "Generating output to keywords/528160.txt\n",
      "Reading articles/540889.txt\n",
      "540889.txt\n",
      "Generating output to keywords/540889.txt\n",
      "Reading articles/546157.txt\n",
      "546157.txt\n",
      "Generating output to keywords/546157.txt\n",
      "Reading articles/549806.txt\n",
      "549806.txt\n",
      "Generating output to keywords/549806.txt\n",
      "Reading articles/553494.txt\n",
      "553494.txt\n",
      "Generating output to keywords/553494.txt\n",
      "Reading articles/553497.txt\n",
      "553497.txt\n",
      "Generating output to keywords/553497.txt\n",
      "Reading articles/559064.txt\n",
      "559064.txt\n",
      "Generating output to keywords/559064.txt\n",
      "Reading articles/560813.txt\n",
      "560813.txt\n",
      "Generating output to keywords/560813.txt\n",
      "Reading articles/571538.txt\n",
      "571538.txt\n",
      "Generating output to keywords/571538.txt\n",
      "Reading articles/602903.txt\n",
      "602903.txt\n",
      "Generating output to keywords/602903.txt\n",
      "Reading articles/609199.txt\n",
      "609199.txt\n",
      "Generating output to keywords/609199.txt\n",
      "Reading articles/612608.txt\n",
      "612608.txt\n",
      "Generating output to keywords/612608.txt\n",
      "Reading articles/613191.txt\n",
      "613191.txt\n",
      "Generating output to keywords/613191.txt\n",
      "Reading articles/620656.txt\n",
      "620656.txt\n",
      "Generating output to keywords/620656.txt\n",
      "Reading articles/622483.txt\n",
      "622483.txt\n",
      "Generating output to keywords/622483.txt\n",
      "Reading articles/656280.txt\n",
      "656280.txt\n",
      "Generating output to keywords/656280.txt\n",
      "Reading articles/668899.txt\n",
      "668899.txt\n",
      "Generating output to keywords/668899.txt\n",
      "Reading articles/668933.txt\n",
      "668933.txt\n",
      "Generating output to keywords/668933.txt\n",
      "Reading articles/698675.txt\n",
      "698675.txt\n",
      "Generating output to keywords/698675.txt\n",
      "Reading articles/702349.txt\n",
      "702349.txt\n",
      "Generating output to keywords/702349.txt\n",
      "Reading articles/738207.txt\n",
      "738207.txt\n",
      "Generating output to keywords/738207.txt\n",
      "Reading articles/740681.txt\n",
      "740681.txt\n",
      "Generating output to keywords/740681.txt\n",
      "Reading articles/771131.txt\n",
      "771131.txt\n",
      "Generating output to keywords/771131.txt\n",
      "Reading articles/771144.txt\n",
      "771144.txt\n",
      "Generating output to keywords/771144.txt\n",
      "Reading articles/771168.txt\n",
      "771168.txt\n",
      "Generating output to keywords/771168.txt\n",
      "Reading articles/778023.txt\n",
      "778023.txt\n",
      "Generating output to keywords/778023.txt\n",
      "Reading articles/802975.txt\n",
      "802975.txt\n",
      "Generating output to keywords/802975.txt\n",
      "Reading articles/80410.txt\n",
      "80410.txt\n",
      "Generating output to keywords/80410.txt\n",
      "Reading articles/81501.txt\n",
      "81501.txt\n",
      "Generating output to keywords/81501.txt\n",
      "Reading articles/833421.txt\n",
      "833421.txt\n",
      "Generating output to keywords/833421.txt\n",
      "Reading articles/84176.txt\n",
      "84176.txt\n",
      "Generating output to keywords/84176.txt\n",
      "Reading articles/84309.txt\n",
      "84309.txt\n",
      "Generating output to keywords/84309.txt\n",
      "Reading articles/86487.txt\n",
      "86487.txt\n",
      "Generating output to keywords/86487.txt\n",
      "Reading articles/86865.txt\n",
      "86865.txt\n",
      "Generating output to keywords/86865.txt\n",
      "Reading articles/880918.txt\n",
      "880918.txt\n",
      "Generating output to keywords/880918.txt\n",
      "Reading articles/904109.txt\n",
      "904109.txt\n",
      "Generating output to keywords/904109.txt\n",
      "Reading articles/90558.txt\n",
      "90558.txt\n",
      "Generating output to keywords/90558.txt\n",
      "Reading articles/937059.txt\n",
      "937059.txt\n",
      "Generating output to keywords/937059.txt\n",
      "Reading articles/957831.txt\n",
      "957831.txt\n",
      "Generating output to keywords/957831.txt\n",
      "Reading articles/98486.txt\n",
      "98486.txt\n",
      "Generating output to keywords/98486.txt\n",
      "Reading articles/99.txt\n",
      "99.txt\n",
      "Generating output to keywords/99.txt\n",
      "Reading articles/99033.txt\n",
      "99033.txt\n",
      "Generating output to keywords/99033.txt\n"
     ]
    }
   ],
   "source": [
    "articles = os.listdir(\"articles\")\n",
    "for article in articles:\n",
    "    print 'Reading articles/' + article\n",
    "    print article\n",
    "    \n",
    "    a = Article(\"https://github.com/samorogu/mineriaTextos/blob/master/CorpusTaggersFInal/documents/\"+article)\n",
    "    try:\n",
    "        a.download()\n",
    "        a.parse()\n",
    "        a.nlp()\n",
    "        authors = a.authors\n",
    "        keywords = a.keywords\n",
    "        \"outputs the keyphrases and summaries to appropriate files\"\n",
    "        print \"Generating output to \" + 'keywords/' + article\n",
    "        keyphraseFile = io.open('keywords_newspaper/' + article, 'wb')\n",
    "        for keywords in keywords:\n",
    "            #keyphraseFile.write(codecs.BOM_UTF16_LE)\n",
    "            keyphraseFile.write(keywords.encode(\"iso-8859-1\") + '\\n')\n",
    "        keyphraseFile.close()\n",
    "\n",
    "    except:\n",
    "        print \"chin\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972067039106\n",
      "1.0\n",
      "0.997765363128\n"
     ]
    }
   ],
   "source": [
    "keywordsRAKE = os.listdir(\"keywordsRAKE\")\n",
    "keywordsTR = os.listdir(\"keywords\")\n",
    "keywords_newspaper = os.listdir(\"keywords_newspaper\")\n",
    "tags=os.listdir(\"tag\")\n",
    "rake=0\n",
    "textrank=0\n",
    "newspap=0\n",
    "for keyword in keywordsTR:\n",
    "    tags = io.open('tag/' + keyword[:-3]+'tags', 'r', encoding=\"iso-8859-1\")\n",
    "    TR = io.open('keywords/' + keyword, 'r', encoding=\"iso-8859-1\")\n",
    "    news = io.open('keywords_newspaper/' + keyword, 'r', encoding=\"iso-8859-1\")\n",
    "    RAKE = io.open('keywordsRAKE/' + keyword, 'r', encoding=\"iso-8859-1\")\n",
    "\n",
    "    TAGS = tags.read()\n",
    "    for i in xrange(0,5):\n",
    "        #text_newspaper=news.readline()\n",
    "        text_rank=TR.readline()\n",
    "        text_rake=RAKE.readline()\n",
    "        text_newspaper=news.readline()\n",
    "\n",
    "        #print text_newspaper\n",
    "        #print \"------------------------------------------\"\n",
    "        #print text\n",
    "      \n",
    "        if TAGS.find(text_rank): \n",
    "            #print \"---->newspaper\"+\"---->\"+linea\n",
    "            textrank=textrank+1\n",
    "        if TAGS.find(text_rake): \n",
    "            #print \"---->newspaper\"+\"---->\"+linea\n",
    "            rake=rake+1  \n",
    "        if TAGS.find(text_newspaper): \n",
    "            newspap=newspap+1       \n",
    "print newspap/(5.0*179) \n",
    "print textrank/(5.0*179) \n",
    "print rake/(5.0*179) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
